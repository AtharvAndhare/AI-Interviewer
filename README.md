# AI Interviewer

The AI Interviewer is a tool that simulates an interview experience, allowing candidates to answer technical questions and receive feedback from an AI-powered interviewer.

## Features

- **Personalized Interview Questions**: Generate 5 detailed and unique technical interview questions based on the candidate's tech stack.
- **Real-time Feedback**: After each question, the AI provides constructive feedback on the candidate's answer.
- **Customizable**: The tool allows you to define your experience, desired position, and tech stack, which tailors the interview to your profile.
- **Tech Stack Flexibility:**: Candidates can specify their tech stack (e.g., Python, Java, Data Science, etc.), and the chatbot will adjust the questions accordingly.
- **Interactive Interface:**:The chatbot allows users to interact by answering questions and receiving feedback in real time.
## Requirements

Ensure the following Python packages are installed:

- **streamlit**: Used for the user interface.
- **huggingface_hub**: Used for integrating with Hugging Face's Inference API to call the AI model for generating interview questions and feedback.
- **requests**: For making HTTP requests.


User Guide:
1) **Start The Applicatoin**: When the app opens, you’ll be greeted with a welcome message.
2) **Enter Your Details:**:Provide your name, email, phone number, years of experience, and the position you’re applying for.
3) **Provide Your Tech Stack:**:Enter the programming languages or technologies you are proficient in (e.g., Python, Java, React, etc.).
4) **Answer Questions:**:The chatbot will ask you 5 technical questions based on your tech stack. After each question, type your answer in the provided text box.
5) **Receive Feedback:**:After submitting your answer, the chatbot will generate feedback based on your response and provide it.
6) **Complete the Interview:**:Once all questions are answered, the chatbot will thank you and inform you about the next steps.

Technical Details:
Libraries Used:
**Streamlit:**: For building the interactive web interface.
Hugging Face Inference Client: To interact with the pre-trained AI models from Hugging Face’s Model Hub.
**JSON:**: For data handling and API response parsing.

Model Details:
**Model: microsoft/Phi-3-mini-4k-instruct**
    A language model from Hugging Face, fine-tuned for text generation and conversational tasks.
    The model is used to generate both interview questions and feedback based on the candidate's responses.

Architectural Decisions:

The application uses a modular architecture with clearly defined functions:
    **call_llm():** A utility function for making requests to the Hugging Face API and getting responses.
    **generate_questions():** Generates 5 interview questions based on the tech stack.
    **generate_response():** Provides feedback on candidate answers.
    Streamlit’s session state is used to maintain question progression and store answers/feedback.


Prompt Design
To ensure that the questions and feedback generated by the AI model are clear, relevant, and useful, the following prompt strategies were used:

Interview Questions Generation: The prompt provided to the model is designed to generate diverse and relevant questions based on the user's tech stack. It ensures the questions are detailed and varied by asking the model to list numbered questions explicitly.

Example prompt: "Generate 5 distinct and detailed technical interview questions for a candidate with skills in Python, Machine Learning, and Data Science."
Answer Feedback: The feedback prompt focuses on evaluating the candidate's answers in terms of clarity, relevance, and communication. The feedback is concise and non-repetitive, ensuring constructive responses.

Example prompt: "Provide a brief, constructive feedback on the following answer: 'I have worked with Python libraries like Pandas and NumPy to handle large datasets.' Focus on clarity, relevance, and communication."
You can install these dependencies with the following command:


Challenges & Solutions:
**Challenge 1: Question Generation**
    Solution: The model was instructed to generate 5 distinct questions for each session. Retry logic was added to ensure unique questions are generated, and any duplicates are removed before displaying them to the user.

**Challenge 2: Model Feedback Accuracy**
    Solution: The feedback prompt was refined to focus on specific aspects of clarity, relevance, and communication. Additional filtering was implemented to remove any prompt echoes or irrelevant instructions from the feedback output.

**Challenge 3: Handling Model API Rate Limits and Latency**
Solution: The application was designed to handle network timeouts gracefully by adding a timeout setting to the InferenceClient. A retry mechanism was implemented to ensure smooth user experience even during temporary network issues.

```bash
pip install -r requirements.txt
